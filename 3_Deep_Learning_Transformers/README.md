# Deep Learning & Transformers

This folder includes deep learning fundamentals, optimization, regularization, transfer learning, and Vision Transformers.

## Subfolders

### Neural_Network_Foundations

- Perceptron, MLP, forward pass, backprop
- Gradient descent, optimizers, learning rate schedules
- Mini-project: MLP on NumPy

### Regularization_Training_Stability

- Bias-variance trade-off, L1/L2 regularization
- Dropout, early stopping, label smoothing
- Data augmentation, mixed precision training

### Transfer_Learning_Pretrained_CNNs

- VGG, ResNet, EfficientNet
- Feature extraction vs fine-tuning
- Training loops, scheduler, validation
- Mini-lab: fine-tune CNN models

### Transformers_for_Vision

- Attention mechanism, multi-head self-attention
- Vision Transformers (ViT) architecture
- Training and throughput vs accuracy trade-offs
